{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIBtoCvK9mxh"
   },
   "source": [
    "# Preparing Data\n",
    "\n",
    "This is largely the same as our Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9MZ75VOJ7zNp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "executionInfo": {
     "elapsed": 4852,
     "status": "ok",
     "timestamp": 1732856572927,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "wKyCypmp71Vv",
    "outputId": "a15ea01f-82c1-49a6-f527-593ec7c4b47e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Term</th>\n",
       "      <th>YearTerm</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Number</th>\n",
       "      <th>Course Title</th>\n",
       "      <th>Sched Type</th>\n",
       "      <th>A+</th>\n",
       "      <th>A</th>\n",
       "      <th>A-</th>\n",
       "      <th>...</th>\n",
       "      <th>B-</th>\n",
       "      <th>C+</th>\n",
       "      <th>C</th>\n",
       "      <th>C-</th>\n",
       "      <th>D+</th>\n",
       "      <th>D</th>\n",
       "      <th>D-</th>\n",
       "      <th>F</th>\n",
       "      <th>W</th>\n",
       "      <th>Primary Instructor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2023-sp</td>\n",
       "      <td>AAS</td>\n",
       "      <td>100</td>\n",
       "      <td>Intro Asian American Studies</td>\n",
       "      <td>DIS</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Shin, Jeongsu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2023-sp</td>\n",
       "      <td>AAS</td>\n",
       "      <td>100</td>\n",
       "      <td>Intro Asian American Studies</td>\n",
       "      <td>DIS</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Shin, Jeongsu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2023-sp</td>\n",
       "      <td>AAS</td>\n",
       "      <td>100</td>\n",
       "      <td>Intro Asian American Studies</td>\n",
       "      <td>DIS</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Lee, Sabrina Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2023-sp</td>\n",
       "      <td>AAS</td>\n",
       "      <td>200</td>\n",
       "      <td>U.S. Race and Empire</td>\n",
       "      <td>LCD</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sawada, Emilia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2023-sp</td>\n",
       "      <td>AAS</td>\n",
       "      <td>215</td>\n",
       "      <td>US Citizenship Comparatively</td>\n",
       "      <td>LCD</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Kwon, Soo Ah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year    Term YearTerm Subject  Number                  Course Title  \\\n",
       "0  2023  Spring  2023-sp     AAS     100  Intro Asian American Studies   \n",
       "1  2023  Spring  2023-sp     AAS     100  Intro Asian American Studies   \n",
       "2  2023  Spring  2023-sp     AAS     100  Intro Asian American Studies   \n",
       "3  2023  Spring  2023-sp     AAS     200          U.S. Race and Empire   \n",
       "4  2023  Spring  2023-sp     AAS     215  US Citizenship Comparatively   \n",
       "\n",
       "  Sched Type  A+   A  A-  ...  B-  C+  C  C-  D+  D  D-  F  W  \\\n",
       "0        DIS   0  11   5  ...   0   0  0   0   0  0   0  1  0   \n",
       "1        DIS   0  17   2  ...   1   0  0   0   0  0   0  0  1   \n",
       "2        DIS   0  13   2  ...   2   0  0   1   0  0   0  1  0   \n",
       "3        LCD   6  15   5  ...   0   0  0   0   0  1   0  1  0   \n",
       "4        LCD  16  12   2  ...   1   0  0   0   0  0   0  0  0   \n",
       "\n",
       "   Primary Instructor  \n",
       "0       Shin, Jeongsu  \n",
       "1       Shin, Jeongsu  \n",
       "2      Lee, Sabrina Y  \n",
       "3      Sawada, Emilia  \n",
       "4        Kwon, Soo Ah  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname=\"../data/uiuc-gpa-dataset.csv\"\n",
    "df=pd.read_csv(fname)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1732856573194,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "UabFGluEJBXn",
    "outputId": "1666bc25-c273-4b6c-c985-c5d56a923f15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAS', '100', '2023-sp', 'DIS', 'Shin, Jeongsu'], dtype='<U25')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_columns = ['Subject', 'Number', 'YearTerm', 'Sched Type', 'Primary Instructor']\n",
    "temp = df[parameter_columns].values\n",
    "X = []\n",
    "for i in range(len(temp)):\n",
    "    X.append([str(temp[i,0]), str(temp[i,1]), str(temp[i,2]), str(temp[i,3]), str(temp[i,4])])\n",
    "X = np.array(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 933,
     "status": "ok",
     "timestamp": 1732856574125,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "85ELCFWLDCy0",
    "outputId": "18c6686c-5dd6-4b37-a4cd-98ccde034e0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.          92.30769231  84.61538462  76.92307692  69.23076923\n",
      "  61.53846154  53.84615385  46.15384615  38.46153846  30.76923077\n",
      "  23.07692308  15.38461538   7.69230769   0.        ]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "grade_columns = ['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D', 'D-', 'F', 'W']\n",
    "grade_percent = np.linspace(100,0, len(grade_columns))\n",
    "print(grade_percent)\n",
    "\n",
    "temp = df[grade_columns].values\n",
    "Z = []\n",
    "for i in range(len(temp)):\n",
    "    score = np.sum(temp[i]*grade_percent)/np.sum(temp[i])\n",
    "    idx = (np.abs(grade_percent - score)).argmin()\n",
    "    Z.append(idx)\n",
    "Z = np.array(Z)\n",
    "print(Z[0])\n",
    "# Prepare data\n",
    "Z_tensor = torch.tensor(Z, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPfxkvqqJ29I"
   },
   "source": [
    "Pre-processing the input data before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "skAFwBLbJ6AQ"
   },
   "outputs": [],
   "source": [
    "# Extract each feature column\n",
    "subject_column = X[:, 0]  #  'AAS'\n",
    "number_column = X[:, 1]   # '100'\n",
    "term_column = X[:, 2]     # '2023-sp'\n",
    "sched_type_column = X[:, 3]  # 'DIS'\n",
    "instructor_column = X[:, 4]  # 'Shin, Jeongsu'\n",
    "\n",
    "# Initialize label encoders for each column\n",
    "subject_encoder = LabelEncoder()\n",
    "number_encoder = LabelEncoder()\n",
    "term_encoder = LabelEncoder()\n",
    "sched_type_encoder = LabelEncoder()\n",
    "instructor_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform each column separately\n",
    "subject_encoded = subject_encoder.fit_transform(subject_column)\n",
    "number_encoded = number_encoder.fit_transform(number_column)\n",
    "term_encoded = term_encoder.fit_transform(term_column)\n",
    "sched_type_encoded = sched_type_encoder.fit_transform(sched_type_column)\n",
    "instructor_encoded = instructor_encoder.fit_transform(instructor_column)\n",
    "\n",
    "# Combine all encoded columns into a single feature array\n",
    "X_encoded = np.column_stack((subject_encoded, number_encoded, term_encoded, sched_type_encoded, instructor_encoded))\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_tensor = torch.tensor(X_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TXUPn_nZDNEs"
   },
   "outputs": [],
   "source": [
    "X_train, X_temp, Z_train, Z_temp = train_test_split(X_tensor, Z_tensor, test_size=0.3, random_state=42)\n",
    "X_val, X_test, Z_val, Z_test = train_test_split(X_temp, Z_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train, Z_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, Z_val), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(X_test, Z_test), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxWkMTIN9q94"
   },
   "source": [
    "# Defining Model\n",
    "\n",
    "We chose to have an embeddings layer for each feature because they're currently one-hot encoded, and an embedding layer maps each category to a low-dimensional vector. We follow this up with 3 fully connected layers with relu activations, with one dropout layer interspersed between the first and second linear layers to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6jrPC1O89BAA"
   },
   "outputs": [],
   "source": [
    "class GradePredictionModel(nn.Module):\n",
    "    def __init__(self, num_classes, vocab_sizes, dropval=0.3, embedding_dim=50, hidden_dim=128):\n",
    "        super(GradePredictionModel, self).__init__()\n",
    "\n",
    "        # Create an embedding for each categorical feature\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(vocab_size, embedding_dim) for vocab_size in vocab_sizes]\n",
    "        )\n",
    "\n",
    "        # Adjust the input size for the fully connected layers\n",
    "        self.fc1 = nn.Linear(embedding_dim * len(vocab_sizes), hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropval)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass each feature through its respective embedding layer\n",
    "        embedded = [embedding(x[:, i]) for i, embedding in enumerate(self.embeddings)]\n",
    "        x = torch.cat(embedded, dim=1)  # Concatenate all embeddings\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0VQMYDS1LXI_"
   },
   "outputs": [],
   "source": [
    "num_classes = len(grade_percent)\n",
    "vocab_sizes = [\n",
    "    len(subject_encoder.classes_),\n",
    "    len(number_encoder.classes_),\n",
    "    len(term_encoder.classes_),\n",
    "    len(sched_type_encoder.classes_),\n",
    "    len(instructor_encoder.classes_)\n",
    "]\n",
    "model = GradePredictionModel(num_classes, vocab_sizes)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af0VGxwxDVXp"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 591285,
     "status": "ok",
     "timestamp": 1732857165698,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "CkbSQVeSDXEJ",
    "outputId": "ab27c814-8738-42da-dad1-eefdd26e9650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.4906, Val Loss: 1.3657\n",
      "Epoch [2/100], Loss: 1.3194, Val Loss: 1.2915\n",
      "Epoch [3/100], Loss: 1.2333, Val Loss: 1.2501\n",
      "Epoch [4/100], Loss: 1.1677, Val Loss: 1.2246\n",
      "Epoch [5/100], Loss: 1.1160, Val Loss: 1.2063\n",
      "Epoch [6/100], Loss: 1.0723, Val Loss: 1.1992\n",
      "Epoch [7/100], Loss: 1.0335, Val Loss: 1.1904\n",
      "Epoch [8/100], Loss: 0.9993, Val Loss: 1.2000\n",
      "Epoch [9/100], Loss: 0.9700, Val Loss: 1.2111\n",
      "Epoch [10/100], Loss: 0.9487, Val Loss: 1.2149\n",
      "Epoch [11/100], Loss: 0.9193, Val Loss: 1.2379\n",
      "Epoch [12/100], Loss: 0.9003, Val Loss: 1.2225\n",
      "Epoch [13/100], Loss: 0.8789, Val Loss: 1.2448\n",
      "Epoch [14/100], Loss: 0.8614, Val Loss: 1.2666\n",
      "Epoch [15/100], Loss: 0.8444, Val Loss: 1.2791\n",
      "Epoch [16/100], Loss: 0.8280, Val Loss: 1.3069\n",
      "Epoch [17/100], Loss: 0.8104, Val Loss: 1.3299\n",
      "Epoch [18/100], Loss: 0.7983, Val Loss: 1.3381\n",
      "Epoch [19/100], Loss: 0.7845, Val Loss: 1.3515\n",
      "Epoch [20/100], Loss: 0.7672, Val Loss: 1.3781\n",
      "Epoch [21/100], Loss: 0.7580, Val Loss: 1.3971\n",
      "Epoch [22/100], Loss: 0.7432, Val Loss: 1.4228\n",
      "Epoch [23/100], Loss: 0.7339, Val Loss: 1.4323\n",
      "Epoch [24/100], Loss: 0.7212, Val Loss: 1.4445\n",
      "Epoch [25/100], Loss: 0.7126, Val Loss: 1.4814\n",
      "Epoch [26/100], Loss: 0.7032, Val Loss: 1.5026\n",
      "Epoch [27/100], Loss: 0.6865, Val Loss: 1.5206\n",
      "Epoch [28/100], Loss: 0.6832, Val Loss: 1.5382\n",
      "Epoch [29/100], Loss: 0.6742, Val Loss: 1.5675\n",
      "Epoch [30/100], Loss: 0.6619, Val Loss: 1.5734\n",
      "Epoch [31/100], Loss: 0.6531, Val Loss: 1.6056\n",
      "Epoch [32/100], Loss: 0.6460, Val Loss: 1.6427\n",
      "Epoch [33/100], Loss: 0.6390, Val Loss: 1.6552\n",
      "Epoch [34/100], Loss: 0.6278, Val Loss: 1.6870\n",
      "Epoch [35/100], Loss: 0.6224, Val Loss: 1.7376\n",
      "Epoch [36/100], Loss: 0.6164, Val Loss: 1.7507\n",
      "Epoch [37/100], Loss: 0.6076, Val Loss: 1.7931\n",
      "Epoch [38/100], Loss: 0.6014, Val Loss: 1.7957\n",
      "Epoch [39/100], Loss: 0.5972, Val Loss: 1.8090\n",
      "Epoch [40/100], Loss: 0.5880, Val Loss: 1.8484\n",
      "Epoch [41/100], Loss: 0.5831, Val Loss: 1.8888\n",
      "Epoch [42/100], Loss: 0.5797, Val Loss: 1.8702\n",
      "Epoch [43/100], Loss: 0.5756, Val Loss: 1.8773\n",
      "Epoch [44/100], Loss: 0.5681, Val Loss: 1.9141\n",
      "Epoch [45/100], Loss: 0.5595, Val Loss: 1.9744\n",
      "Epoch [46/100], Loss: 0.5621, Val Loss: 1.9519\n",
      "Epoch [47/100], Loss: 0.5519, Val Loss: 2.0380\n",
      "Epoch [48/100], Loss: 0.5487, Val Loss: 1.9954\n",
      "Epoch [49/100], Loss: 0.5446, Val Loss: 2.0739\n",
      "Epoch [50/100], Loss: 0.5382, Val Loss: 2.0811\n",
      "Epoch [51/100], Loss: 0.5343, Val Loss: 2.1032\n",
      "Epoch [52/100], Loss: 0.5311, Val Loss: 2.1359\n",
      "Epoch [53/100], Loss: 0.5251, Val Loss: 2.1639\n",
      "Epoch [54/100], Loss: 0.5231, Val Loss: 2.1752\n",
      "Epoch [55/100], Loss: 0.5183, Val Loss: 2.1774\n",
      "Epoch [56/100], Loss: 0.5130, Val Loss: 2.2655\n",
      "Epoch [57/100], Loss: 0.5079, Val Loss: 2.2612\n",
      "Epoch [58/100], Loss: 0.5090, Val Loss: 2.2806\n",
      "Epoch [59/100], Loss: 0.5058, Val Loss: 2.3097\n",
      "Epoch [60/100], Loss: 0.4953, Val Loss: 2.2918\n",
      "Epoch [61/100], Loss: 0.4965, Val Loss: 2.3610\n",
      "Epoch [62/100], Loss: 0.4951, Val Loss: 2.3714\n",
      "Epoch [63/100], Loss: 0.4925, Val Loss: 2.3977\n",
      "Epoch [64/100], Loss: 0.4836, Val Loss: 2.4412\n",
      "Epoch [65/100], Loss: 0.4853, Val Loss: 2.4119\n",
      "Epoch [66/100], Loss: 0.4812, Val Loss: 2.4178\n",
      "Epoch [67/100], Loss: 0.4774, Val Loss: 2.5300\n",
      "Epoch [68/100], Loss: 0.4771, Val Loss: 2.5006\n",
      "Epoch [69/100], Loss: 0.4742, Val Loss: 2.5295\n",
      "Epoch [70/100], Loss: 0.4691, Val Loss: 2.5104\n",
      "Epoch [71/100], Loss: 0.4627, Val Loss: 2.6101\n",
      "Epoch [72/100], Loss: 0.4671, Val Loss: 2.5735\n",
      "Epoch [73/100], Loss: 0.4628, Val Loss: 2.6330\n",
      "Epoch [74/100], Loss: 0.4594, Val Loss: 2.6234\n",
      "Epoch [75/100], Loss: 0.4586, Val Loss: 2.6455\n",
      "Epoch [76/100], Loss: 0.4548, Val Loss: 2.6745\n",
      "Epoch [77/100], Loss: 0.4532, Val Loss: 2.7692\n",
      "Epoch [78/100], Loss: 0.4488, Val Loss: 2.7280\n",
      "Epoch [79/100], Loss: 0.4476, Val Loss: 2.7536\n",
      "Epoch [80/100], Loss: 0.4436, Val Loss: 2.7310\n",
      "Epoch [81/100], Loss: 0.4419, Val Loss: 2.7685\n",
      "Epoch [82/100], Loss: 0.4411, Val Loss: 2.7991\n",
      "Epoch [83/100], Loss: 0.4365, Val Loss: 2.8059\n",
      "Epoch [84/100], Loss: 0.4384, Val Loss: 2.8393\n",
      "Epoch [85/100], Loss: 0.4352, Val Loss: 2.8546\n",
      "Epoch [86/100], Loss: 0.4342, Val Loss: 2.8774\n",
      "Epoch [87/100], Loss: 0.4371, Val Loss: 2.9248\n",
      "Epoch [88/100], Loss: 0.4344, Val Loss: 2.8572\n",
      "Epoch [89/100], Loss: 0.4267, Val Loss: 2.9066\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, Z_batch in train_loader:\n",
    "          if Z_batch.ndim > 1:\n",
    "            Z_batch = Z_batch.argmax(dim=1)\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(X_batch)\n",
    "          loss = loss_fn(outputs, Z_batch)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "          for X_batch, Z_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_fn(outputs, Z_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "\n",
    "train_model(model, train_loader, val_loader, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFGIzBrmDij6"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "For evaluation, we came up with a metric for \"Relaxed accuracy.\" This was because if the ground truth was an A+, but the model predicted an A, it would be treated as \"wrong\", even though a prediction of A is still pretty good. This metric uses the grade difference to score predictions based on how close they are to the true label. The closer the prediction, the higher the score.\n",
    "\n",
    "The math behind this metric is we first calculate the difference (so an A+ is much \"closer\" to an A, with a distance of 1, than it is to an F, with a distance of 12). We then divide this by the \"max distance,\" or the maximum one-hot encoding of the labels (there are 14 labels, labelled 0 to 13). The maximum possible difference is 13. So the difference divided by the maximum difference shows a \"percent\" error. So if the label was A+, and we predicted A, we got an accuracy of 12/13. Whereas if we predicted W, we got an accuracy of 0. Thus, this relaxed accuracy shows us a much clearer picture.\n",
    "\n",
    "Thus, while we only predict around 50% of the labels exactly, we have around 96% accuracy, meaning most of our predictions were very close. This number is similar to if we exactly predicted 50% of the labels, were only plus/minus (A to A+, B to B-, etc.) off on 40%, and were two grades off (A+ to A-, A to B+, A- to B, etc.) on the remaining 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1732857165886,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "ZpYQd8rnDbnY",
    "outputId": "bafcd44a-4704-4c83-a672-3271d500916e"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    relaxed_score = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Z_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total += Z_batch.size(0)\n",
    "            correct += (predicted == Z_batch).sum().item()\n",
    "\n",
    "            grade_diff = torch.abs(predicted - Z_batch)\n",
    "            max_distance = len(grade_columns) - 1\n",
    "            relaxed_score += (1 - (grade_diff / max_distance)).sum().item()\n",
    "\n",
    "    relaxed_accuracy = 100 * relaxed_score / total\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Relaxed Test Accuracy: {relaxed_accuracy:.2f}%\")\n",
    "    return accuracy, relaxed_accuracy\n",
    "\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XudnwdZig73G"
   },
   "source": [
    "# Mini Batch Analysis\n",
    "We have tried out 5 different batch sizes (16, 32, 64, 128, 256) for our mini-batch learning. We ran the training for these different batch sizes for a fewer number of epochs just enough so we could understand which one was providing better results. We've plotted our findings below.\n",
    "\n",
    "Our conlusion from this analysis is that although each of the mini-batch sizes produced slight variations in accuracies, the variation was insignificant. Thus the mini-batch size had little impact on our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYjhpmtlb0hu"
   },
   "outputs": [],
   "source": [
    "batch_sizes = [16, 32, 64, 128, 256]\n",
    "train_loaders = []\n",
    "val_loaders = []\n",
    "test_loaders = []\n",
    "for batch_size in batch_sizes:\n",
    "    train_loaders.append(DataLoader(TensorDataset(X_train, Z_train), batch_size=batch_size, shuffle=True))\n",
    "    val_loaders.append(DataLoader(TensorDataset(X_val, Z_val), batch_size=batch_size))\n",
    "    test_loaders.append(DataLoader(TensorDataset(X_test, Z_test), batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 518143,
     "status": "ok",
     "timestamp": 1732857684027,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "c9yj9Tbts4qq",
    "outputId": "992450de-9b96-47e9-b706-465a66079945"
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "errors = []\n",
    "for i in range(len(batch_sizes)):\n",
    "    print(\"Training with batch size:\", batch_sizes[i])\n",
    "    model = GradePredictionModel(num_classes, vocab_sizes)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_model(model, train_loaders[i], val_loaders[i], loss_fn, optimizer, 10)\n",
    "    acc, err = evaluate(model, test_loaders[i])\n",
    "    accuracies.append(acc)\n",
    "    errors.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1732857684466,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "wzxEYK4BifaK",
    "outputId": "c9b469c0-926d-46b9-c52b-ef69c4ba25cb"
   },
   "outputs": [],
   "source": [
    "relaxed_accuracies = errors\n",
    "plt.plot(batch_sizes, accuracies, label=\"Strict Accuracy\")\n",
    "plt.plot(batch_sizes, relaxed_accuracies, label=\"Relaxed Accuracy\")\n",
    "# plt.ylim((47,52))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Accuracy %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypMAQyeRlxsQ"
   },
   "source": [
    "# Testing Different Optimizers\n",
    "In order to find the best optimizer to use, we tested out 3 of the most popular optimizers for our application: Adam, Stochastic Gradient Descent, and AdaGrad.\n",
    "\n",
    "From our analysis, we found far better results for the Adam optimizer than either of the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129797,
     "status": "ok",
     "timestamp": 1732857814261,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "IE9lh5J-l5MD",
    "outputId": "090918bf-ce11-401c-a2c6-d43f7f47db8d"
   },
   "outputs": [],
   "source": [
    "opt_accuracies = []\n",
    "opt_relaxed = []\n",
    "\n",
    "# Adam Optimizer\n",
    "print(\"Training with Adam Optimizer:\")\n",
    "model = GradePredictionModel(num_classes, vocab_sizes)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model(model, train_loader, val_loader, loss_fn, optimizer, 10)\n",
    "acc, rel = evaluate(model, test_loader)\n",
    "opt_accuracies.append(acc)\n",
    "opt_relaxed.append(rel)\n",
    "\n",
    "# SGD Optimizer\n",
    "print(\"Training with SGD Optimizer:\")\n",
    "model = GradePredictionModel(num_classes, vocab_sizes)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "train_model(model, train_loader, val_loader, loss_fn, optimizer, 10)\n",
    "acc, rel = evaluate(model, test_loader)\n",
    "opt_accuracies.append(acc)\n",
    "opt_relaxed.append(rel)\n",
    "\n",
    "# AdaGrad Optimizer\n",
    "print(\"Training with AdaGrad Optimizer:\")\n",
    "model = GradePredictionModel(num_classes, vocab_sizes)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.001)\n",
    "train_model(model, train_loader, val_loader, loss_fn, optimizer, 10)\n",
    "acc, rel = evaluate(model, test_loader)\n",
    "opt_accuracies.append(acc)\n",
    "opt_relaxed.append(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1732857814582,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "Biv0FtE9n3eg",
    "outputId": "6aaae109-f55e-4e45-bf7e-9a18a5aecdcd"
   },
   "outputs": [],
   "source": [
    "optimizers = [\"Adam\", \"SGD\", \"AdaGrad\"]\n",
    "plt.bar(optimizers, opt_accuracies, width=0.4, label=\"Strict Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Accuracy %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ju3LS5_yqNam"
   },
   "source": [
    "# Other Hyperparameters\n",
    "We have conducted a further analysis of 2 main hyperperameters: dropout rate and learning rate.\n",
    "\n",
    "For the dropout rate we tested 5 different values 0.0, 0.1, 0.3, 0.5, 0.8 and plotted the resulting model accuracies. We found that dropout rates between 0.1 and 0.5 performed slightly better than the rest.\n",
    "\n",
    "Next we tested 3 different learning rates: 0.1, 0.01, 0.001. Here we see that 0.1 resulted in far lower accuracy than the other 2. We concluded that 0.1 was too large of a rate, and 0.001 slightly outperformed 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291674,
     "status": "ok",
     "timestamp": 1732858106252,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "X4BfzW0UpTCh",
    "outputId": "7a40f2c9-9850-4db0-9150-385988908c0b"
   },
   "outputs": [],
   "source": [
    "dropouts = [0.0, 0.1, 0.3, 0.5, 0.8]\n",
    "accuracies_drop = []\n",
    "relaxed_drop = []\n",
    "for i in range(len(dropouts)):\n",
    "    print(\"Training with dropout value:\", dropouts[i])\n",
    "    model = GradePredictionModel(num_classes, vocab_sizes, dropouts[i])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, 10)\n",
    "    acc, rel = evaluate(model, test_loader)\n",
    "    accuracies_drop.append(acc)\n",
    "    relaxed_drop.append(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1732858106441,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "Lb7jZreMqmQA",
    "outputId": "5468a94b-5ee7-4edc-87d7-b2c2882d61f0"
   },
   "outputs": [],
   "source": [
    "plt.plot(dropouts, accuracies_drop, label=\"Strict Accuracy\")\n",
    "plt.ylim((45,55))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Dropout rate\")\n",
    "plt.ylabel(\"Accuracy %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1732858106758,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "g24B9lPesLX3",
    "outputId": "36731408-1a9a-421d-e1e7-c355a6ecb963"
   },
   "outputs": [],
   "source": [
    "plt.plot(dropouts, relaxed_drop, label=\"Relaxed Accuracy\")\n",
    "plt.ylim((90, 100))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Dropout rate\")\n",
    "plt.ylabel(\"Accuracy %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206579,
     "status": "ok",
     "timestamp": 1732858313334,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "nlcmWD_VsUFk",
    "outputId": "5ef81059-97b4-40de-efc9-c06a4a8ab595"
   },
   "outputs": [],
   "source": [
    "rates = [0.1, 0.01, 0.001]\n",
    "accuracies_rate = []\n",
    "relaxed_rate = []\n",
    "for i in range(len(rates)):\n",
    "    print(\"Training with learning rate:\", rates[i])\n",
    "    model = GradePredictionModel(num_classes, vocab_sizes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=rates[i])\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, 10)\n",
    "    acc, rel = evaluate(model, test_loader)\n",
    "    accuracies_rate.append(acc)\n",
    "    relaxed_rate.append(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56212,
     "status": "ok",
     "timestamp": 1732858885128,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "8pJQDa-Bs3Y6",
    "outputId": "851516f0-e6fb-4e26-c723-a5e5ca818327"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "final_model = GradePredictionModel(num_classes,  vocab_sizes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
    "train_model(final_model, train_loader, val_loader, criterion, optimizer, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1732860025116,
     "user": {
      "displayName": "Sunskar Dhanker",
      "userId": "08080050347218288056"
     },
     "user_tz": 480
    },
    "id": "PmPQbWA-Sppv",
    "outputId": "12ab2a0c-f133-4e45-fd75-6df40da22180"
   },
   "outputs": [],
   "source": [
    "\n",
    "save_path = '../model/grade_prediction_model.pth'\n",
    "\n",
    "\n",
    "# Save the entire model\n",
    "torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
